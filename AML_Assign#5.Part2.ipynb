{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4b054-7575-4cc6-9b9d-9572354abce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee67271-3bfa-4406-bd4a-86202fe4f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Synthetic Time Series Data\n",
    "# I'll start by generating a synthetic time series dataset with a mix of trend, seasonality, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72e630-8c3d-4a71-9e96-3f0d3a36a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_points=1000):\n",
    "    # Time variable (index for each point in series)\n",
    "    time = np.arange(0, n_points)\n",
    "\n",
    "    # Trend component (a small increase over time)\n",
    "    trend = 0.01 * time\n",
    "\n",
    "    # Seasonal component (a sine wave pattern)\n",
    "    seasonal = 0.5 * np.sin(0.1 * time)\n",
    "\n",
    "    # Noise component (random fluctuations)\n",
    "    noise = 0.2 * np.random.randn(n_points)\n",
    "\n",
    "    # Combined series\n",
    "    series = trend + seasonal + noise\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba6219-1305-4998-a390-84e54804ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate initial time series data with 1000 points\n",
    "n_points = 1000\n",
    "series = generate_synthetic_data(n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe08b8-77e5-43d1-b999-cdff6970a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated time series data\n",
    "plt.plot(series)\n",
    "plt.title(\"Synthetic Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdf4e8-4557-46a9-bd75-ab41a61c2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize and Prepare Data for Modeling\n",
    "# I'll normalize the data and prepare it for time series modeling by creating sequences with a specified window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc51846-f569-44b3-98f4-5ff4c35e4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the series\n",
    "scaler = MinMaxScaler()\n",
    "series_scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2c677-5517-48db-8402-f58ad2433107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sequence length (window size) and batch size\n",
    "sequence_length = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55686a-f71e-43ef-bf82-68adb6920e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the scaled series into training and test sets based on indices\n",
    "split_index = int(len(series_scaled) * 0.8)\n",
    "train_series = series_scaled[:split_index]\n",
    "test_series = series_scaled[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36974f-f2df-4280-95cd-fa4216c51f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences for training and testing sets\n",
    "train_generator = TimeseriesGenerator(train_series, train_series, length=sequence_length, batch_size=batch_size)\n",
    "test_generator = TimeseriesGenerator(test_series, test_series, length=sequence_length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2620fe-b73f-4cd5-83b7-a4a60e1ba095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Different RNN Models\n",
    "# Now, I'll define four different types of RNN models to compare: LSTM, GRU, BiDirectional RNN, and a simple Deep RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317de0e6-7f09-4abb-a295-7cf01673cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df154896-1ea3-4c93-8929-223a429206cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a GRU model\n",
    "def create_gru_model():\n",
    "    model = Sequential([\n",
    "        GRU(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852aa11b-1555-45f1-9ab5-a11092d7c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Bidirectional LSTM model\n",
    "def create_bidirectional_model():\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(50, activation='relu'), input_shape=(sequence_length, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65757a-6c56-4bc2-a29b-882d71cf87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Deep RNN model with two layers\n",
    "def create_deep_rnn_model():\n",
    "    model = Sequential([\n",
    "        SimpleRNN(50, activation='relu', return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "        SimpleRNN(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2482d2a-9bf2-4fc8-8fae-74d1b5418804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train and Evaluate Each Model on Initial Dataset\n",
    "# I'll now train each model on the initial dataset and record the test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5d02c-999e-4bae-9eb1-a91b7cc9fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model, train_gen, test_gen, epochs=10):\n",
    "    # Train model with specified epochs\n",
    "    model.fit(train_gen, epochs=epochs, validation_data=test_gen)\n",
    "\n",
    "    # Evaluate on test data and return the loss\n",
    "    loss = model.evaluate(test_gen)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859c30e-7e14-4c3e-90b4-01edbacf3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models in a dictionary for easy reference\n",
    "models = {\n",
    "    \"LSTM\": create_lstm_model(),\n",
    "    \"GRU\": create_gru_model(),\n",
    "    \"BiDirectional RNN\": create_bidirectional_model(),\n",
    "    \"Deep RNN\": create_deep_rnn_model()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891dd1f-559d-4fca-926d-2e6eb76b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model and record the performance on the initial dataset\n",
    "results_initial = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model on initial dataset\")\n",
    "    loss = train_and_evaluate_model(model, train_generator, test_generator)\n",
    "    results_initial[name] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b5a16-a972-45a1-87f9-28b6cd4dec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Increase Dataset Size by 3x and Repeat Experiments\n",
    "# I’ll create a new dataset that’s three times larger, repeating the data generation process with more points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152165e-22bb-48ab-ab2c-fee5ccc7e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger dataset with 3000 points\n",
    "series_large = generate_synthetic_data(n_points * 3)\n",
    "series_large_scaled = scaler.fit_transform(series_large.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56519a-7c5c-4961-ae6c-8b782f6db5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the larger dataset into training and test sets\n",
    "split_index_large = int(len(series_large_scaled) * 0.8)\n",
    "train_series_large = series_large_scaled[:split_index_large]\n",
    "test_series_large = series_large_scaled[split_index_large:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0310327-3e19-41f0-a57a-3d74f7d2de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences for the larger dataset\n",
    "train_generator_large = TimeseriesGenerator(train_series_large, train_series_large, length=sequence_length, batch_size=batch_size)\n",
    "test_generator_large = TimeseriesGenerator(test_series_large, test_series_large, length=sequence_length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889a558-9d1f-4a63-853f-bbf7fdf55ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model on the larger dataset and record performance\n",
    "results_large = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model on larger dataset\")\n",
    "    loss = train_and_evaluate_model(model, train_generator_large, test_generator_large)\n",
    "    results_large[name] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883864c-1cfd-4765-ab9b-7b2100e60b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compare Results\n",
    "# Finally, I'll print the results to compare how each model performed on the smaller and larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa2001-42de-4951-937a-c99a480bdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results on Initial Dataset (1000 points):\")\n",
    "for name, loss in results_initial.items():\n",
    "    print(f\"{name}: Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0d414-0ec2-4b46-953c-6aa63d79d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResults on Larger Dataset (3000 points):\")\n",
    "for name, loss in results_large.items():\n",
    "    print(f\"{name}: Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acc06f-f135-49cb-bbba-3cecbe5dfdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
